{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GeneratingIndianFaces.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GeniGaus/100DaysOfMLCode/blob/master/GeneratingIndianFaces.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "H7PY19Vmy-Yz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generating Indian Faces with Deconvolutional Networks\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Hq_R8gxlh3K0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 1: Creating Indian Face Database\n",
        "\n",
        "---\n",
        "Taking reference of *Yale Face Extended Database* and *Radbound Faces Database*, I created an **Indian Face Database** consisting of images of 28 Indians in different poses and lighting. Each individual has been clicked in a light background in the following 8 poses:\n",
        "> 1. Angry\n",
        "> 2. Contemptuous\n",
        "> 3. Disgusted\n",
        "> 4. Fearful\n",
        "> 5. Happy\n",
        "> 6. Neutral\n",
        "> 7. Sad\n",
        "> 8. Surprised\n",
        "\n",
        "> ![Different Poses](https://raw.githubusercontent.com/GeniGaus/MLBlr/master/assets/IndFaces1.jpg)\n",
        ">>>>>> **Different Poses**\n",
        "\n",
        "Each pose has been clicked in 2 different lighting condition:\n",
        "> 1. Ambient\n",
        "> 2. Dim light\n",
        "\n",
        "> ![Different lighting](https://raw.githubusercontent.com/GeniGaus/MLBlr/master/assets/IndFaces2.jpg)\n",
        ">>>>>>> **Different Lighting Conditions**\n",
        "\n",
        "Each picture is of $530 \\times 730$ pixels dimension in jpeg format. Each picture is labelled as **IndAxx_Pxx_Lxx**, where xx are *placeholders* for individual *identity number, pose number* and *lighting condition number* respectively. For example, an image of individual with identity number 1 and clicked in a happy pose in ambient lighting condition is labelled as IndA01_P05_L01; where '01' after IndA refers to identity number, 'P05' denotes pose 5 which is 'happy' pose and 'L01' refers to lighting condition 1 which is ambient.\n",
        "\n",
        "All the images corresponding to a particular individual are placed in one folder labelled as **IndAxx** where xx is placeholder for identity number.\n",
        "\n",
        "All the folders related to all the individuals are placed inside another folder named **IndianFacesA**."
      ]
    },
    {
      "metadata": {
        "id": "kAV-f1XUiB5B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Fundamental Concept\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "ZT55hP4bzCXO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The idea of generating faces with deconvolutional networks is based on the paper [Learning to generate chairs, tables and cars with convolutional networks](https://arxiv.org/abs/1411.5928). This paper shows that if a neural network is trained using a dataset consisting of model's identity, view, and transformation parameters as input and the image as output, the following results are obtained:\n",
        "\n",
        "> 1) As expected, the network could learn all examples by heart and generate them. That is, the network learns to generate 2D projections from high-level description of 3D models.\n",
        "\n",
        "> 2) Unexpectedly, the network seemed to learn something also.\n",
        "\n",
        "> *   It was able to learn concepts about 3D space.\n",
        "> *   It was able to learn object structure and transfer that knowledge within the object class. This was evident from the fact that it was able to infer the remaining unseen viewpoints of the object.\n",
        "> *   It was able to interpolate between objects of the same and of different types.\n",
        "> *   It was able to randomly generate objects of new styles.\n",
        "\n",
        "\n",
        "Here, this idea will be applied to train the network to generate and interpolate between Indian faces from a high-level description of image as input."
      ]
    },
    {
      "metadata": {
        "id": "vpq4D-eyzNpK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2: Creating the Model\n",
        "\n",
        "### Model Description\n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "khWmuy3ozOr6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Input\n",
        "\n",
        "The network takes as input, a high-level description of the image, which include:\n",
        "> *  Identity vector, c\n",
        "> *  Pose vector of face, p\n",
        "> *  Lighting vector, l\n",
        "\n",
        "Thus, the dataset is $D = \\{(c_1,p_1,l_1),...,(c_n,p_n,l_n)\\}$\n",
        "\n",
        "\n",
        "### Output\n",
        "\n",
        "The output of the network is $O = \\{x_1,...,x_n \\}$, where x is the RGB image of face."
      ]
    },
    {
      "metadata": {
        "id": "0aQ3a-Ew1DQ3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Transformation of Indian Face Database to Network-inputtable Form\n",
        "\n",
        "Instead of writing code to fetch image data in training or model creation part of the code, we can write a separate class for the database to minimize code changes in case of database or network architecture changes and to increase reusability. \n",
        "\n",
        "The database which is a collection of images is represented by the **\"IndianInstances\"** class. To represent each image in the database, we create a class **\"IndianInstance\"**. These two classes help in transforming the database into a form which can be passed to the network as training inputs and outputs.\n",
        "\n",
        "The **\"IndianInstances\"** class takes the directory location of the database as input and stores it as an attribute *directory*. In our case, since we stored it in drive with the name \"IndianFacesA\", we would pass the value \"drive/IndianFacesA\". \n",
        "\n",
        "It would then fetch all subfolders and create an instance of **\"IndianInstance\"** for each image. In **\"IndianInstance\"** class, we create training input and output matrices as follows:\n",
        "\n",
        "\n",
        "**Formation of training input**\n",
        "\n",
        "Since all the images were labelled with identity number, pose number and lighting condition number. We can take the image's label and form input from that label as follows:\n",
        "\n",
        "> 1. Extract identity number from image label and transform that number into one-hot encoding. This encoded array will be passed as identity vector, c in input.\n",
        "\n",
        "> 2. Extract pose number from image label and transform that number into one-hot encoding. This encoded array will be passed as pose vector, p in input.\n",
        "\n",
        "> 3. Extract lighting condition number from image label and transform that number into one-hot encoding. This encoded array will be passed as lighting vector, l in input.\n",
        "\n",
        "\n",
        "**Formation of training output**\n",
        "\n",
        "The image itself will be the output. The matrices of pixel values of the image in 3 channels(RGB) will be the training output."
      ]
    },
    {
      "metadata": {
        "id": "6KUap5pqzwC1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Network Architecture\n",
        "\n",
        "The next question that we ask ourselves would be what should the network look like. Specifically speaking, how many and what layers should be there. In order to get the intution, let's consider a classification network. In classification models, we would usually map an image to a class(i.e., a high level description of the object). The classification network consist of convolution layers followed by pooling and then fully connected layers for mapping. Below figure shows the classification network.\n",
        "\n",
        "\n",
        "\n",
        "> ![Classification network](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-08-at-2-26-09-am.png?w=748)\n",
        "\n",
        "\n",
        "\n",
        "So, in order to generate image from a high-level description, we can perform the above steps in reverse.\n",
        "\n",
        "> 1) First, each of the 3 inputs is passed through a fully connected layer. Then, they are combined and passed through a fully connected layer.\n",
        "\n",
        "> 2) Then we need to expand or unpool the result of above layers to increase their dimensionality. For this, upsampling is performed, in which each entry of a feature map is replaced by an $s \\times s$ block with entry value in top-left corner and dummy values( usually 0) elsewhere. This increases the width and height of feature map s times. For our network, we will take $s=2$.\n",
        "\n",
        "> 3) Then convolution is performed.\n",
        "\n",
        "Combining 2) and 3) would seem as if we have dotted the grid with dabs of paint and then spread and mixed them using convolution kernels. Upsampling and convolution is together usually referred to as \"upconvolution\". \n",
        "\n",
        "> ![Upconvolution](https://zo7.github.io/img/2016-09-25-generating-faces/deconv.png)\n",
        "\n",
        "---\n",
        "**Upconvolution Decoded**\n",
        "\n",
        "\n",
        "![Upconv](https://raw.githubusercontent.com/GeniGaus/MLBlr/master/assets/upconv.gif)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Adding another convolution after upconvolution has experimentally been shown to improve the quality of generated images. Together, they will be referred here as deconvolution layer.\n",
        "\n",
        "\n",
        "This above deductions give rise to the \"1s-S-deep\" model from the paper(as shown below in figure 1)  which is used to build the network.\n",
        "\n",
        "> ![1s-S-model of Chairs tables](https://zo7.github.io/img/2016-09-25-generating-faces/chairs-model.png)\n",
        ">>>>>>**Figure1: 1s-S-model of Chairs tables experiment**\n",
        "\n",
        "\n",
        "The transformed model(in Figure2) which we will be using for our purpose will have the segmentation part removed. \n",
        "\n",
        "\n",
        "> ![1s-S-model for Indian faces](https://raw.githubusercontent.com/GeniGaus/MLBlr/master/assets/DeconvFaceArch.jpg)\n",
        ">>>>>>**Figure2**\n",
        "\n",
        "In our network, all fully connected(FC) layers are replaced by convolution layers so that the input size need not be fixed. Since here FC layers are applied to inputs, we need to first reshape the input because convolution require 3 dimensions while the input has 1 dimension. The inputs are reshaped from(len) to (height, width, num_channel) dimension where len is respective length of each input vector, height=1,width=1 and num_channel=len. Then, convolution is performed on each input with 512 filters. These are combined and then convolved using 1024 filters.\n",
        "\n",
        "For our network, we will use 5 deconvolution layer excluding the output layer because that is the maximum number we can use in Google Colab without exhausting the resources. In each layer, $2 \\times 2$ upsampling is performed followed by $5 \\times 5$ convolution, together forming upconvolution layer, followed by another $3 \\times 3$ convolution. The output of every convolution is passed through LeakyReLU to introduce non-linearity in the network. Batch normalization is performed at the end in each of the deconvolution layer to ensure LeakyReLU activations behave properly.\n",
        "\n",
        "In the last deconvolution layer, a $3 \\times 3$ convolution is performed with 3 filters after the upconvolution+convolution, in order to create a 3-channel output image.\n"
      ]
    },
    {
      "metadata": {
        "id": "T9CkDkmaz5zm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3: Training the Model"
      ]
    },
    {
      "metadata": {
        "id": "HnxO_BKgvk8q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Training with 50 epochs\n",
        "\n",
        "train_model(\"drive/IndianFacesA (871e7db6)\",\n",
        "            \"drive/IndianFacesAOutput\", \n",
        "             num_epochs= 50\n",
        "            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v2Hs4Y7fKZgR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Training with 100 epochs\n",
        "\n",
        "train_model(\"drive/IndianFacesA (871e7db6)\",\n",
        "            \"drive/IndianFacesAOutput\", \n",
        "             num_epochs= 100\n",
        "            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XQg9cfswtY9k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Training with 200 epochs\n",
        "\n",
        "train_model(\"drive/IndianFacesA\",\n",
        "            \"drive/IndianFacesAOutput\", \n",
        "             num_epochs= 200\n",
        "            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4xF6ZkKQ9_kq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Training for 300 epochs\n",
        "\n",
        "train_model(\"drive/IndianFacesA\",\n",
        "            \"drive/IndianFacesAOutput\", \n",
        "             num_epochs= 300\n",
        "            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g0T1370Tqvwn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Training for 500 epochs\n",
        "\n",
        "train_model(\"drive/IndianFacesA\",\n",
        "            \"drive/IndianFacesAOutput\", \n",
        "             num_epochs= 500\n",
        "            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "77CHSXjhLFFK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Training for 600 epochs\n",
        "\n",
        "train_model(\"drive/IndianFacesA\",\n",
        "            \"drive/IndianFacesAOutput\", \n",
        "             num_epochs= 600\n",
        "            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yQegV32o8ho7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Training for 500 epochs with stochastic gradient descent\n",
        "\n",
        "train_model(\"drive/IndianFacesA\",\n",
        "            \"drive/IndianFacesAOutput\", \n",
        "             num_epochs= 500,\n",
        "             optimizer='sgd'\n",
        "            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5QMssFlZoGdp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 4: Generate Faces from Trained Model\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "UKCWtimvq009",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Image Generation\n",
        "-----\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "xu_0I7ATuhTC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generate_from_yaml(\"drive/single2.yaml\", \"drive/IndianFacesAOutput/FaceGen.IndianFaces.model.d5.e50.adam.h5\", \"drive/IndianFacesAGeneratedOutput_d5_adam_e50_Single2\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O_kqjNeZ5vl5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generate_from_yaml(\"drive/single.yaml\", \"drive/IndianFacesAOutput/FaceGen.IndianFaces.model.d5.e200.adam.h5\", \"drive/IndianFacesAGeneratedOutput_d5_adam_e200_Single\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pgKHaHlNqDiD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generate_from_yaml(\"drive/single.yaml\", \"drive/IndianFacesAOutput/FaceGen.IndianFaces.model.d5.e300.adam.h5\", \"drive/IndianFacesAGeneratedOutput_d5_adam_e300_Single\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a5GaeJCsEt6y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generate_from_yaml(\"drive/single.yaml\", \"drive/IndianFacesAOutput/FaceGen.IndianFaces.model.d5.e500.adam.h5\", \"drive/IndianFacesAGeneratedOutput_d5_adam_epoch500_Single\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0mb8w0FbmJP8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generate_from_yaml(\"drive/single.yaml\", \"drive/IndianFacesOutput/FaceGen.IndianFaces.model.d5.e600.adam.h5\", \"drive/IndianFacesGeneratedOutput_Single_e600\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jY7QpZfrmXDX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generate_from_yaml(\"drive/random.yaml\", \"drive/IndianFacesAOutput/FaceGen.IndianFaces.model.d5.e600.adam.h5\", \"drive/IndianFacesAGeneratedOutput_d5_adam_epoch600_Random\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6Dq81tEJJ-Aj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generate_from_yaml(\"drive/interpolate.yaml\", \"drive/IndianFacesAOutput/FaceGen.IndianFaces.model.d5.e500.adam.h5\", \"drive/IndianFacesAGeneratedOutput_d5_adam_epoch500_Interpolate_Id2\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YEHZjlZOJ54Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generate_from_yaml(\"drive/single.yaml\", \"drive/IndianFacesOutput/FaceGen.IndianFaces.model.d5.e300.sgd.h5\", \"drive/IndianFacesGeneratedOutput_Single_sgd_e300\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gGVzU7DsnMMs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generate_from_yaml(\"drive/single.yaml\", \"drive/IndianFacesAOutput/FaceGen.IndianFaces.model.d5.e500.sgd.h5\", \"drive/IndianFacesAGeneratedOutput_d5_sgd_epoch500_Single\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zfdZWdyQbuUp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Analysis of Result\n",
        "\n",
        "----\n",
        "\n",
        "1. The same network architecture can generate faces from different origin or nationality. The same network which was used to generate yale, radboud, jaffe faces can now generate Indian faces.\n",
        "\n",
        "2. As we increase the number of epochs for which it is trained, the clarity of the images increase.\n",
        "\n",
        "Following are the single images generated from the trained model:\n",
        "\n",
        "> 1. **Trained for 5 epochs:**\n",
        ">> ![alt text](https://raw.githubusercontent.com/GeniGaus/MLBlr/master/assets/GenSingleImageEpoch5.jpg)\n",
        "\n",
        "> 2. **Trained for 10 epochs:**\n",
        ">> ![alt text](https://raw.githubusercontent.com/GeniGaus/MLBlr/master/assets/GenSingleImageEpoch10.jpg)\n",
        "\n",
        "> 3. **Trained for 20 epochs:**\n",
        ">> ![alt text](https://raw.githubusercontent.com/GeniGaus/MLBlr/master/assets/GenSingleImageEpoch20.jpg)\n",
        "\n",
        "> 4. **Trained for 50 epochs:**\n",
        ">> ![alt text](https://raw.githubusercontent.com/GeniGaus/MLBlr/master/assets/SingleImageEpoch50.jpg)\n",
        "\n",
        "> 5. **Trained for 100 epochs:**\n",
        ">> ![alt text](https://raw.githubusercontent.com/GeniGaus/MLBlr/master/assets/SingleImageEpoch100.jpg)\n",
        "\n",
        "> 6. **Trained for 200 epochs:**\n",
        ">> ![alt text](https://raw.githubusercontent.com/GeniGaus/MLBlr/master/assets/SingleImageEpoch200.jpg)\n",
        "\n",
        "> 7. **Trained for 300 epochs:**\n",
        ">>![alt text](https://raw.githubusercontent.com/GeniGaus/MLBlr/master/assets/SingleImageEpoch300.jpg)\n",
        "\n",
        "> 8. **Trained for 500 epochs:**\n",
        ">> ![alt text](https://raw.githubusercontent.com/GeniGaus/MLBlr/master/assets/GenSingleImageEpoch500.jpg)\n",
        "\n",
        "\n",
        "3. If we change the optimizer, we would get different images from partially trained network. With Adam, it seems as if the image has been blurred out. With stochastic gradient as optimizer, the image seems as an abstract painting.\n",
        "\n",
        "> 1. **Trained for 50 epochs:**\n",
        ">>![Sgd](https://raw.githubusercontent.com/GeniGaus/MLBlr/master/assets/IndianFacesGeneratedOutput_Single_sgd_e300.jpg)\n",
        "\n",
        "> 2. **Trained for 500 epochs:**\n",
        ">> ![alt text](https://raw.githubusercontent.com/GeniGaus/MLBlr/master/assets/SingleImageEpoch500sgd.jpg)\n"
      ]
    },
    {
      "metadata": {
        "id": "KrCmrcCtUvUs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Areas to dig further\n",
        "\n",
        "----\n",
        "\n",
        "1. I would like to see the result of the network on training with combined data of all databases, i.e., people from different origin, race and nationality.\n",
        "\n",
        "2. Here I have used identity, pose and lighting as parameters. If I had more time, I would have tried with orientation and occlusion parameters. Then, I would have tried with more than 3 parameters and with all of identity, pose, lighting, orientation and occlusion parameters combined. Then, compare each of their results and generate face with unseen orientation and occlusion parameters.\n",
        "\n",
        "3. Train with other models specified in the paper and compare their results.\n",
        "\n",
        "4. In the paper, it was mentioned that the effect of data augmentation is the same as increasing the training set size. Both lead to generalization of features but bad reconstruction of finer details. I wanted to test this by adding augmentation and increasing the training set size of any individual and compare the results.\n",
        "\n",
        "5. Expanding this theory of generalization of features by data augmentation, can we generalize features of people belonging to a particular family? I would like to have tested this theory by including another parameter family and trained the network to see the result. This idea can be further expanded to race and nationality."
      ]
    }
  ]
}